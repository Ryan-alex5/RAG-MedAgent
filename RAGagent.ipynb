{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import gradio as gr\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    print(\"OPENAI_API_KEY not found in .env file. Please set it.\")\n",
    "    exit()\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "pdf_path = \"KnowledgeBase/testEndometriosis.pdf\" # <--- IMPORTANT: CHANGE THIS TO YOUR PDF FILE\n",
    "\n",
    "# --- Imports for document handling and RAG ---\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "# --- Global variable for the retriever ---\n",
    "# This will hold our document retriever so the chat function can access it.\n",
    "retriever = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file found. Loading and processing...\n",
      "PDF split into 314 chunks.\n",
      "Creating vector store...\n",
      "Vector store and retriever created successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. LOAD AND PROCESS THE PDF (Function) ---\n",
    "# We put this in a function to keep the code clean.\n",
    "def process_pdf(path):\n",
    "    global retriever\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Error: PDF file not found at {path}\")\n",
    "        return None\n",
    "\n",
    "    print(\"PDF file found. Loading and processing...\")\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    print(f\"PDF split into {len(docs)} chunks.\")\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    \n",
    "    print(\"Creating vector store...\")\n",
    "    vector_store = FAISS.from_documents(docs, embeddings)\n",
    "    \n",
    "    # Set the global retriever variable\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "    print(\"Vector store and retriever created successfully.\")\n",
    "\n",
    "# --- Process the initial PDF on startup ---\n",
    "process_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. BUILD THE CONVERSATIONAL RAG CHAIN ---\n",
    "\n",
    "### MODIFIED ### - This whole section is new and replaces the old `rag_chain`.\n",
    "# This chain is more complex to handle chat history.\n",
    "\n",
    "# a) First, a chain to condense the user's question and chat history into a standalone question.\n",
    "# This is so the retriever gets a good query even if the user asks a follow-up question.\n",
    "condense_question_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model_name=MODEL, temperature=0.7)\n",
    "\n",
    "# Chain to create the standalone question\n",
    "condense_question_chain = condense_question_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# b) The main chain that answers the question using the retrieved context.\n",
    "answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ou are a helpful and knowledgeable assistant for medical students. Answer the user’s question using only the information provided in the context below, which is sourced from official medical guidelines and references. If the answer is not clearly supported by the context, respond with “I don’t know based on the provided information.” Do not make assumptions or provide information beyond what is stated in the context.\\n\\nContext:\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# This function determines which question to use for retrieval (original or condensed)\n",
    "def get_retrieval_question(input_dict):\n",
    "    if input_dict.get(\"chat_history\"):\n",
    "        return condense_question_chain\n",
    "    else:\n",
    "        return input_dict[\"question\"]\n",
    "\n",
    "# c) The final conversational RAG chain.\n",
    "# It chains together the question condensation, document retrieval, and final answer generation.\n",
    "conversational_rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=get_retrieval_question | retriever \n",
    "    )\n",
    "    | answer_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from faiss-cpu) (24.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n",
      "Vector store created successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. CREATE EMBEDDINGS AND VECTOR STORE ---\n",
    "# Import components for embeddings and vector storage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create an embeddings model instance. This will convert our text chunks into numerical vectors.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create a FAISS vector store from the document chunks and their embeddings.\n",
    "# This store allows for very fast similarity searches.\n",
    "if 'docs' in locals() and docs:\n",
    "    print(\"Creating vector store...\")\n",
    "    vector_store = FAISS.from_documents(docs, embeddings)\n",
    "    print(\"Vector store created successfully.\")\n",
    "    # Create a retriever from the vector store. The retriever's job is to fetch relevant documents.\n",
    "    retriever = vector_store.as_retriever()\n",
    "else:\n",
    "    print(\"No documents were loaded. Skipping vector store creation.\")\n",
    "    retriever = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Launching Gradio Interface...\n",
      "Open the URL in your browser to chat with your RAG agent.\n",
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 4. CREATE AND LAUNCH THE GRADIO INTERFACE ---\n",
    "\n",
    "### MODIFIED ### - The chat function now handles history.\n",
    "def chat_function(message, history):\n",
    "    \"\"\"The function that powers the Gradio chat interface.\"\"\"\n",
    "    if retriever is None:\n",
    "        return \"Sorry, the document could not be loaded. Please check the PDF file path and restart.\"\n",
    "\n",
    "    # Convert Gradio's history format to LangChain's message format\n",
    "    # Gradio history is a list of lists: [['user_msg_1', 'bot_msg_1'], ['user_msg_2', 'bot_msg_2']]\n",
    "    chat_history_for_chain = []\n",
    "    for user_msg, ai_msg in history:\n",
    "        chat_history_for_chain.append(HumanMessage(content=user_msg))\n",
    "        chat_history_for_chain.append(AIMessage(content=ai_msg))\n",
    "\n",
    "    # Invoke the conversational RAG chain with the message and history\n",
    "    response = conversational_rag_chain.invoke({\n",
    "        \"question\": message,\n",
    "        \"chat_history\": chat_history_for_chain\n",
    "    })\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"\\nLaunching Gradio Interface...\")\n",
    "print(\"Open the URL in your browser to chat with your RAG agent.\")\n",
    "\n",
    "ui = gr.ChatInterface(\n",
    "    fn=chat_function,\n",
    "    title=\"Antenatal Care AI\",\n",
    "    description=\"Ask any question the topic. The agent remembers your conversation.\",\n",
    "    examples=[\n",
    "    [\"What are the recommended antenatal visits during pregnancy?\"],\n",
    "    [\"How is preeclampsia identified and managed antenatally?\"],\n",
    "    [\"What supplements should be given during antenatal care?\"],\n",
    "    [\"What are the key components of a first-trimester check-up?\"]\n",
    "]\n",
    ")\n",
    "ui.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
